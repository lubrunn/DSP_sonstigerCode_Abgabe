library(openxlsx)
library(XML)
library(glue)
library(tidyverse)
library(magrittr)
library(httr)
library(rvest)
library(stringr)
library(dplyr)
library(tm)
library(xml2)
library(anytime)
testit <- function(x)
{
p1 <- proc.time()
Sys.sleep(x)
proc.time() - p1
}
lastdate <- as.Date("2018-11-30")
help <- rev(seq(lastdate,Sys.Date(), by = "days"))
dates <- as.numeric(as.POSIXct(c(help[seq(1,length(help),135)],lastdate)))
dates
dates <- as.numeric(as.POSIXct(c(help[seq(1,length(help))],lastdate)))
dates
dates <- as.numeric(as.POSIXct(c(help[seq(1,length(help),135)],lastdate)))
datalist <- list()
for (j in 1:(length(dates) - 1)) {
url <- paste("https://finance.yahoo.com/quote/", VIX, "/history?period1=",dates[j + 1],"&period2=",dates[j],"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
datalist[[j]] <- readHTMLTable(nodes[[1]])
testit(10)
}
for (j in 1:(length(dates) - 1)) {
url <- paste("https://finance.yahoo.com/quote/", "VIX", "/history?period1=",dates[j + 1],"&period2=",dates[j],"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
datalist[[j]] <- readHTMLTable(nodes[[1]])
testit(10)
}
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
readHTMLTable(nodes[[1]])
url <- paste("https://finance.yahoo.com/quote/%5EVIX/history?period1=1543536000&period2=1613952000&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
url <- paste("https://finance.yahoo.com/quote/%5EVIX/history?period1=1543536000&period2=1613952000&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
readHTMLTable(nodes[[1]])
datalist <- readHTMLTable(nodes[[1]])
df_stock <- as.data.frame(do.call(rbind, datalist))
View(df_stock)
df_stock <- apply(df_stock,2,as.character)
df_stock
datalist <- readHTMLTable(nodes[[1]])
View(datalist)
View(datalist)
View(datalist)
testit <- function(x)
{
p1 <- proc.time()
Sys.sleep(x)
proc.time() - p1
}
lastdate <- as.Date("2018-11-30")
help <- rev(seq(lastdate,Sys.Date(), by = "days"))
dates <- as.numeric(as.POSIXct(c(help[seq(1,length(help),135)],lastdate)))
for (i in indeces) {
datalist <- list()
for (j in 1:(length(dates) - 1)) {
url <- paste("https://au.finance.yahoo.com/quote/%5E",i,"/history?period1=",dates[j + 1],"&period2=", dates[j],"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
datalist[[j]] <- readHTMLTable(nodes[[1]])
testit(2)
}
}
for (j in 1:(length(dates) - 1)) {
url <- paste("https://au.finance.yahoo.com/quote/%5E",i,"/history?period1=",dates[j + 1],"&period2=", dates[j],"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
datalist[[j]] <- readHTMLTable(nodes[[1]])
testit(2)
}
}
datalist <- list()
for (j in 1:(length(dates) - 1)) {
url <- paste("https://au.finance.yahoo.com/quote/%5E","VIX","/history?period1=",dates[j + 1],"&period2=", dates[j],"&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true",sep = "")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
nodes <- getNodeSet(html, "//table")
datalist[[j]] <- readHTMLTable(nodes[[1]])
testit(2)
}
}
df_stock <- as.data.frame(do.call(rbind, datalist))
View(df_stock)
df_stock <- apply(df_stock,2,as.character)
View(df_stock)
#csv
write.csv(df_stock,paste0("C:/Users/simon/OneDrive - UT Cloud/Eigene Dateien/Data/Twitter/sentiment/Model/VIX.csv"),row.names = F )
install.packages("gtrendsR")
library (gtrendsR)
keywords=c("coronavirus","Covid","Covid-19","corona")
country=c('DE','EN')
time=("2018-11-30 2021-02-22")
library(ggplot2)
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
country=c('EN')
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
country=c('US','DE')
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
channel='web'
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
keywords=c("coronavirus")
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
time=("2018-11-30 2021-02-20")
time=("2018-11-30 2021-02-20")
keywords=c("coronavirus","Covid","Covid-19","corona")
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
country=c('US')
time=("2018-11-30 2021-02-20")
channel='web'
trends = gtrends(keywords, gprop =channel,geo=country, time = time )
trends = gtrends(keyword = c("coronavirus","Covid","Covid-19","corona"),
gprop=c("web", "news"),geo="en-US", time = time )
trends = gtrends(keyword = c("coronavirus","Covid","Covid-19","corona"),
gprop=c("web", "news"),hl="en-US", time = time )
trends = gtrends(keyword = c("coronavirus","Covid","Covid-19","corona"),
gprop=c("web"),hl="en-US", time = time )
res <- gtrends(c("nhl", "nba"), geo = c("CA", "US"))
library(devtools)
devtools::install_github("PMassicotte/gtrendsR")
